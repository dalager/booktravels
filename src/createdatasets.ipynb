{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load goodreads data\n",
    "- Package imports\n",
    "- Load data\n",
    "- Data cleaning\n",
    "- save as `01_cleaned_goodreads_data.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sourcefolder = '../data/sources'\n",
    "outputfolder = '../data/outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load goodreads data, clean whitespace from author names and filter out unread books\n",
    "df = pd.read_csv(sourcefolder + \"/goodreads_library_export.csv\")\n",
    "\n",
    "df[\"Author\"] = df[\"Author\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "df[\"Author\"].unique()\n",
    "df = df[df[\"Read Count\"] > 0]\n",
    "\n",
    "# collapse the list on author and add book count and average 'Average Rating'\n",
    "# the title should be kept as a list for further processing\n",
    "df = df.groupby(\"Author\").agg({\"Title\": list, \"Average Rating\": \"mean\"}).reset_index()\n",
    "\n",
    "df[\"Books\"] = df[\"Title\"]\n",
    "df.drop(\"Title\", axis=1, inplace=True)\n",
    "df[\"Book Count\"] = df[\"Books\"].apply(len)\n",
    "\n",
    "df.to_csv(outputfolder + \"/01_cleaned_goodreads_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Author country lookup\n",
    "We will use the WikiData SPARQL api to get the country of the authors.\n",
    "This is the best I can find.\n",
    "And we need to correct the country of the authors in the dataset manually with overrides afterwards.\n",
    "\n",
    "The author countries is saved to `02_author_countries.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to do SPARQL lookup in wikidata.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def execute_query(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s (christian@dalager.com)\" % (\n",
    "        sys.version_info[0],\n",
    "        sys.version_info[1],\n",
    "    )\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "def get_author_countries(author_name):\n",
    "    author_name = re.sub(r\"\\s+\", \" \", author_name)\n",
    "    author_name = author_name.strip()\n",
    "\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    query = (\n",
    "        \"\"\"SELECT ?countryLabel WHERE {\n",
    "        ?author ?label \"%s\"@en.\n",
    "        ?author wdt:P27 ?country.\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "        }\"\"\"\n",
    "        % author_name\n",
    "    )\n",
    "\n",
    "    results = execute_query(endpoint_url, query)\n",
    "\n",
    "    bindings = results[\"results\"][\"bindings\"]\n",
    "    countries = list({entry[\"countryLabel\"][\"value\"] for entry in bindings})\n",
    "\n",
    "    return countries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the lookup\n",
    "It will take around 1-2 minutes for 370 authors\n",
    "And the data will be saved in a csv file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting 369 authors\n"
     ]
    }
   ],
   "source": [
    "# extract all author names from \"Author\" to a unique list\n",
    "\n",
    "df = pd.read_csv(outputfolder+'/01_cleaned_goodreads_data.csv')\n",
    "\n",
    "authors = df['Author'].unique()\n",
    "\n",
    "print(f'Requesting {len(authors)} authors')\n",
    "\n",
    "# Create a list to collect author and their countries\n",
    "author_country_list = []\n",
    "\n",
    "# Iterate over all authors and get the countries  (1.5 min)\n",
    "for author in authors:\n",
    "    countries = get_author_countries(author)  # Make sure this returns a list of countries\n",
    "    author_country_list.append({'Author': author, 'Countries': countries})\n",
    "\n",
    "df_authorcountries = pd.DataFrame(author_country_list)\n",
    "    \n",
    "# save to csv\n",
    "df_authorcountries.to_csv(outputfolder+'/02_author_countries.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begynd at arbejde med listerne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Manually located authors' countries in the dataset\n",
    "\n",
    "The wikipedia data is not perfect, so we need to manually correct the data.\n",
    "I have a correction list for my authors in `/data/sources/author_country_corrections.csv`\n",
    "\n",
    "Saves the corrected list as `04_country_corrected_author.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df_authorcountries = pd.read_csv(outputfolder+'/02_author_countries.csv')\n",
    "df_authorcountries['Countries'] = df_authorcountries['Countries'].apply(ast.literal_eval)\n",
    "\n",
    "df_manually_placed = pd.read_csv(sourcefolder+\"/author_country_corrections.csv\")\n",
    "df_manually_placed['Countries'] = df_manually_placed['Countries'].apply(ast.literal_eval)\n",
    "\n",
    "df_manually_placed.rename(\n",
    "    columns={\"Countries\": \"Countries_Manually_Placed\"}, inplace=True\n",
    ")\n",
    "merged_df = pd.merge(\n",
    "    df_authorcountries,\n",
    "    df_manually_placed[[\"Author\", \"Countries_Manually_Placed\"]],\n",
    "    on=\"Author\",\n",
    "    how=\"left\",\n",
    "    validate=\"1:1\",\n",
    ")\n",
    "# Replace the 'Countries' column with the corrected countries where available\n",
    "merged_df[\"Countries\"] = merged_df[\"Countries_Manually_Placed\"].fillna(\n",
    "    merged_df[\"Countries\"]\n",
    ")\n",
    "\n",
    "# Drop the 'Countries_Manually_Placed' column if you don't need it anymore\n",
    "df_authorcountries = merged_df.drop(columns=[\"Countries_Manually_Placed\"])\n",
    "\n",
    "df_authorcountries.to_csv(outputfolder+'/04_country_corrected_authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the variants of country names to the standard names\n",
    "\n",
    "The country names from wikidata are standardized, so we need to do that.\n",
    "\n",
    "Outputs `05_author_countries_standardized.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from old country names to new country names\n",
    "country_mapping = {\n",
    "    'Ancient Rome': 'Italy',\n",
    "    'Austria-Hungary': 'Austria',\n",
    "    'Cisleithania': 'Austria',\n",
    "    'Classical Athens': 'Greece',\n",
    "    'Czechoslovakia': 'Czech Republic',\n",
    "    'Dutch Republic': 'Netherlands',\n",
    "    'England': 'United Kingdom',\n",
    "    'German Empire': 'Germany',\n",
    "    'Kingdom of Denmark': 'Denmark',\n",
    "    'Kingdom of England': 'United Kingdom',\n",
    "    'Kingdom of Great Britain': 'United Kingdom',\n",
    "    'Kingdom of Italy': 'Italy',\n",
    "    'Kingdom of the Netherlands': 'Netherlands',\n",
    "    'Nazi Germany': 'Germany',\n",
    "    'Russian Empire': 'Russia',\n",
    "    'Russian Socialist Federative Soviet Republic': 'Russia',\n",
    "    'Russian Soviet Federative Socialist Republic': 'Russia',\n",
    "    'Scotland': 'United Kingdom',\n",
    "    'United Kingdom of Great Britain and Ireland': 'United Kingdom',\n",
    "    'Ireland':'Republic of Ireland',\n",
    "    'Weimar Republic': 'Germany',\n",
    "}\n",
    "\n",
    "# Function to clean country names\n",
    "def clean_country_list(country_list):\n",
    "    replaced= [country_mapping.get(country, country) for country in country_list if country_mapping.get(country, country) is not None]\n",
    "    return list(set(replaced))\n",
    "\n",
    "# Apply the function to the 'Countries' column\n",
    "df_authorcountries['Countries'] = df_authorcountries['Countries'].apply(clean_country_list)\n",
    "\n",
    "df_authorcountries.to_csv(outputfolder+'/05_author_countries_standardized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender enriched data\n",
    "\n",
    "I have manually added gender-info to the authors in the dataset, saved in `author_genders.csv`.\n",
    "\n",
    "The data was not i wikipedia and given the short list of authors, I have manually added gender information to the authors.\n",
    "\n",
    "This is in many ways not perfect, and even though I have tried to be as thorough as possible, there might be som errors.\n",
    "\n",
    "The gender information applied is binary, which will not represent the authors own perspective in all cases, and that is not the intention either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genders = pd.read_csv(sourcefolder+'/author_genders.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete the final dataset\n",
    "\n",
    "Combining my booklist with country and gender information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authorcountries = pd.read_csv(outputfolder+'/05_author_countries_standardized.csv')\n",
    "df_genders = pd.read_csv(sourcefolder+'/author_genders.csv')\n",
    "df_mybooks = pd.read_csv(outputfolder+'/01_cleaned_goodreads_data.csv')\n",
    "\n",
    "df_final = pd.merge(df_mybooks, df_authorcountries, on='Author', how='left', validate='1:1')\n",
    "df_final = df_final.merge(df_genders, on='Author', how='left', validate='1:1')\n",
    "\n",
    "df_final['Countries'] = df_final['Countries'].apply(ast.literal_eval)\n",
    "df_final['Books'] = df_final['Books'].apply(ast.literal_eval)\n",
    "\n",
    "df_final.to_csv(outputfolder+'/05_final_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
